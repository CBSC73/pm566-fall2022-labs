---
title: "Homework 3"
author: "CB"
date: "`r Sys.Date()`"
output: github_document
always_allow_html: true
---


```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
### Load required libraries
```{r, results='hide'}
library(data.table)
library(dplyr)
library(tidyverse)
library(dtplyr)
library(knitr)
library(ggplot2)
library(rvest)
library(httr)
library(xml2)
library(stringr)
```
## Part 1: APIs 

```{r}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2%20trial%20vaccine")

# Finding the counts

counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]")

counts <- as.character(counts)
counts

# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
```
### _There are 3,997 results from this search_

### Create database
```{r}
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db      = "pubmed",
    term    = "sars-cov-2 trial vaccine",
    retmax  = 250
  ), 
)
```
```{r}
# Extracting, make character
ids <- httr::content(query_ids)
ids <- as.character(ids)
```


```{r}
ids <- stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. 
ids <- stringr::str_remove_all(ids, "</?Id>")
head(ids)
```

```{r}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db  = "pubmed",
    id  = paste(ids,collapse = ","),
  retmax = 250,
  rettype = "abstract"
    )
)
# Extracting the content of the response of GET
publications <- httr::content(publications)

publications_txt <- as.character(publications)

```

### Create a dataset containing Pubmed ID number,Title of the paper,Name of the journal where it was published,Publication date, and Abstract of the paper (if any).
```{r}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)

```
```{r, get titles}
titles <- str_extract(pub_char_list, "<ArticleTitle>[[:print:][:space:]]+</ArticleTitle>")
titles[[1]]
titles <- str_remove_all(titles, "</?[[:alnum:]- =\"]+>")
titles[[1]]
```

```{r, get name of journal}
journalname <- str_extract(pub_char_list, "<Title>[[:print:][:space:]]+</Title>")
journalname[[1]]
journalname <- str_remove_all(journalname, "</?[[:alnum:]- =\"]+>")
journalname[[1]]

```

```{r, get abstracts}
abstracts <- str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts[[1]]
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]- =\"]+>") 
abstracts[[1]]
abstracts <- str_replace_all(abstracts, "[[:space:]]+"," ")
abstracts[[1]]

```
```{r, get publication year}
pubyear <- str_extract(pub_char_list, "<Year>[0-9]{4}</Year>")
pubyear[[1]]
pubyear <- str_remove_all(pubyear, "</?[[:alnum:]- =\"]+>")
pubyear[[1]]

```
```{r, get publication month}
pubmonth <- str_extract(pub_char_list, "<Month>[a-zA-Z]{3}</Month>")
pubmonth[[1]]
pubmonth <- str_remove_all(pubmonth, "</?[[:alnum:]- =\"]+>")
pubmonth[[1]]

```

```{r, get publication day}
pubday <- str_extract(pub_char_list, "<Day>[0-9]{1,2}</Day>")
pubday[[1]]
pubday <- str_remove_all(pubday, "</?[[:alnum:]- =\"]+>")
pubday[[1]]
```
### Make a table with the dataset information 

```{r}

#Create publication date variable
pubdate <- paste(pubmonth, pubday, pubyear, sep="-")
pubdate[[1]]
  
```
```{r}
#Create table with all elements
database <- data.frame(
  PubMedId = ids,
  Title    = titles,
  Abstract = abstracts,
  Journal =journalname,
  Publication_Date=pubdate
  
)
knitr::kable(database[1:10,], caption = "Covid-19 Vaccine Trials Papers")
```

## Part 2: Text mining

```{r}
#Obtain dataset, save to computer then load in (unable to load directly from website without issues)

library(tidytext)
pubmedabs <- read.csv("C:\\Users\\clair\\Desktop\\PM566\\pubmed.csv")
str(pubmedabs)
```

```{r}
library(forcats)

```
### Step 1: Tokenize the abstracts and count the number of each token
```{r}
#Count number of tokens
pubmedabs %>%
  unnest_tokens(word, abstract) %>%
  count(word, sort = TRUE) %>% 
  top_n(10, n)
  
```

```{r}
#Look at top tokens 
pubmedabs %>%
  unnest_tokens(word, abstract) %>%
  #anti_join(stop_words, by = c("word")) %>%
  count(word, sort = TRUE)  %>%
  top_n(20, n) %>%
  ggplot(aes(n, fct_reorder(word, n))) +
  geom_col()
```
### _Figure showing the top 20 words in the abstracts. "COVID" and "19" are at the top of the list, thus likely articles about COVID are the most common abstracts. "Cancer" and "prostate" are also on the list and most likely the second most common articles after COVID._ 

```{r}
#Remove stop words
pubmedabs %>%
  unnest_tokens(word, abstract) %>%
  count(word, sort = TRUE) %>%
  anti_join(stop_words, by = c("word")) %>%
  top_n(20, n) %>%
  ggplot(aes(n, fct_reorder(word, n))) +
  geom_col()

```
### _Figure showing the top 20 words AFTER removing stop words. This DOES change what tokens appear as the most frequent, and now the results are more illuminating. COVID is at the top so most likely these are the most numerous articles. Then prostate cancer second. Pre-eclampsia is now on the list so that may be the third most common article subject. The terms "women" and "pregnancy" are probably high becuase these are article about pre-eclampsia which is a pregnancy disorder._

```{r}
#Top 5 words table 

top5 <- 
    pubmedabs %>%
  unnest_tokens(word, abstract) %>%
  count(word, sort = TRUE) %>% 
  anti_join(stop_words, by = c("word")) 

top5<- top5[1:5,]

top5 %>%
    arrange(desc(n)) %>%
knitr::kable(caption = "Table - Top 5 Words", align=c("l", "c"))

```

### Examine Top 5 words BY search term after removing stop words
```{r}
top5byterm <- 
    pubmedabs %>%
    group_by(term) %>% 
  unnest_tokens(word, abstract) %>%
  count(word, sort = TRUE) %>% 
  anti_join(stop_words, by = c("word")) 

top5byterm %>%
    top_n(5,n) %>% 
    arrange(desc(term)) %>%
knitr::kable(caption = "Table - Top 5 Words by Search Term", align=c("l", "c","c"))
```

### _Table showing the top 5 terms from each search term abstract. We can see that the top terms are very different depending on the original search term. We get more words that are more specific to that particular topic._


## Step 2: Tokenize the abstracts into bigrams. Find the 10 most common bigrams and visualize them with ggplot2.
```{r}
pubmedabs %>%
  unnest_ngrams(bigram, abstract, n=2) %>%
  count(bigram, sort = TRUE) %>%
  top_n(10, n) %>%
  ggplot(aes(n, fct_reorder(bigram, n))) +
  geom_col()+
  labs(title="Top Ten Most Common Two Word Terms")

```

### _ Figure showing the top ten most common two word terms ("bigrams") in the abstracts. Covid-19 is likely the most common abstract subject, followed by prostate cancer and pre-eclampsia._

### Step 3: Calculate the TF-IDF value for each word-search term combination. (here you want the search term to be the “document”) What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?
```{r}
#Check number of terms in the dataset to confirm 
search_term <- 
    pubmedabs %>%
    count(term)

search_term %>%
    arrange(desc(n)) %>%
knitr::kable()
```

```{r}
term_table <-pubmedabs %>%
  group_by(term) %>% 
  unnest_tokens(word, abstract) %>%
  count(word, sort = TRUE) %>%
    bind_tf_idf(word, term, n)



term_table %>%
    top_n(5,tf_idf) %>% 
    arrange(desc(term)) %>%
knitr::kable(digits =4, align=c("l", "c", "c", "c","c","c"), caption = "Table - Top 5 TF-IDFs by Search Term")

```

### Table showing the highest TD-IDF values for each of the five search terms.
### _Comparing the information in this table to the top tokens from Question 1, 




